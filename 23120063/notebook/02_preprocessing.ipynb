{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Tiá»n Xá»­ LÃ½ Dá»¯ Liá»‡u\n",
    "## HR Analytics: Job Change of Data Scientists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import ThÆ° Viá»‡n vÃ  Dá»¯ Liá»‡u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "BASE_DIR = os.path.abspath(\"..\")\n",
    "sys.path.append(BASE_DIR)\n",
    "# ThÃªm path Ä‘á»ƒ import modules\n",
    "\n",
    "# Import custom modules - CHá»ˆ cÃ¡c hÃ m preprocessing\n",
    "from src.data_processing import (\n",
    "    load_data, handle_missing_values, detect_and_remove_outliers,\n",
    "    normalize_features, feature_engineering, encode_categorical_features,\n",
    "    compute_descriptive_statistics, validate_data_values\n",
    ")\n",
    "\n",
    "from src.visualization import (\n",
    "    plot_numeric_feature_distribution, plot_categorical_feature_distribution,\n",
    "    plot_correlation_heatmap, setup_plot_style\n",
    ")\n",
    "\n",
    "print(\"Táº¥t cáº£ thÆ° viá»‡n Ä‘Ã£ Ä‘Æ°á»£c import thÃ nh cÃ´ng!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dá»¯ Liá»‡u ÄÃ£ PhÃ¢n TÃ­ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dá»¯ liá»‡u\n",
    "file_path = \"../data/raw/aug_train.csv\"\n",
    "raw_data = load_data(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Validate Dá»¯ Liá»‡u Ban Äáº§u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate dá»¯ liá»‡u ban Ä‘áº§u\n",
    "print(\"=== VALIDATE Dá»® LIá»†U BAN Äáº¦U ===\")\n",
    "initial_validation = validate_data_values(raw_data)\n",
    "\n",
    "# TÃ³m táº¯t missing values\n",
    "print(\"\\nğŸ“Š TÃ“M Táº®T MISSING VALUES BAN Äáº¦U:\")\n",
    "total_missing = 0\n",
    "for col_name, col_info in initial_validation.items():\n",
    "    if col_info.get('missing', 0) > 0:\n",
    "        missing_count = col_info['missing']\n",
    "        percentage = (missing_count / len(raw_data)) * 100\n",
    "        print(f\"   â€¢ {col_name:25}: {missing_count:>3} values ({percentage:5.1f}%)\")\n",
    "        total_missing += missing_count\n",
    "\n",
    "if total_missing == 0:\n",
    "    print(\"   âœ… KhÃ´ng cÃ³ missing values\")\n",
    "else:\n",
    "    total_percentage = (total_missing / (len(raw_data) * len(raw_data.dtype.names))) * 100\n",
    "    print(f\"   ğŸ“ˆ Tá»•ng missing values: {total_missing} ({total_percentage:.2f}% dataset)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Xá»­ LÃ½ Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xá»­ lÃ½ missing values\n",
    "print(\"=== Xá»¬ LÃ MISSING VALUES ===\")\n",
    "\n",
    "# Chiáº¿n lÆ°á»£c xá»­ lÃ½ cho tá»«ng loáº¡i cá»™t\n",
    "strategy_dict = {\n",
    "    'city_development_index': 'median',\n",
    "    'gender': 'mode', \n",
    "    'enrolled_university': 'mode',\n",
    "    'education_level': 'mode',\n",
    "    'major_discipline': 'mode',\n",
    "    'experience': 'mode',\n",
    "    'company_size': 'mode',\n",
    "    'company_type': 'mode',\n",
    "    'last_new_job': 'mode'\n",
    "}\n",
    "\n",
    "print(\"Chiáº¿n lÆ°á»£c xá»­ lÃ½ missing values:\")\n",
    "for col, strategy in strategy_dict.items():\n",
    "    if col in raw_data.dtype.names:\n",
    "        print(f\"   â€¢ {col:25}: {strategy}\")\n",
    "\n",
    "# Ãp dá»¥ng xá»­ lÃ½ missing values\n",
    "data_no_missing = handle_missing_values(raw_data, strategy='mean')\n",
    "\n",
    "# Kiá»ƒm tra káº¿t quáº£\n",
    "print(\"\\nâœ… KIá»‚M TRA SAU KHI Xá»¬ LÃ MISSING VALUES:\")\n",
    "validation_after_missing = validate_data_values(data_no_missing)\n",
    "\n",
    "remaining_missing = 0\n",
    "for col_name, col_info in validation_after_missing.items():\n",
    "    if col_info.get('missing', 0) > 0:\n",
    "        remaining_missing += col_info['missing']\n",
    "        print(f\"   âš ï¸  {col_name:25}: {col_info['missing']} values cÃ²n missing\")\n",
    "\n",
    "if remaining_missing == 0:\n",
    "    print(\"   ğŸ‰ Táº¤T Cáº¢ MISSING VALUES ÄÃƒ ÄÆ¯á»¢C Xá»¬ LÃ!\")\n",
    "else:\n",
    "    print(f\"   ğŸ“Š CÃ²n {remaining_missing} missing values cáº§n xá»­ lÃ½ thÃªm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PhÃ¡t Hiá»‡n vÃ  Xá»­ LÃ½ Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PhÃ¡t hiá»‡n vÃ  xá»­ lÃ½ outliers\n",
    "print(\"=== PHÃT HIá»†N VÃ€ Xá»¬ LÃ OUTLIERS ===\")\n",
    "\n",
    "# Sá»­ dá»¥ng IQR method\n",
    "data_no_outliers, outlier_mask = detect_and_remove_outliers(data_no_missing, method='iqr')\n",
    "\n",
    "print(f\"ğŸ“Š Káº¾T QUáº¢ OUTLIER DETECTION:\")\n",
    "print(f\"   â€¢ Sá»‘ lÆ°á»£ng outliers phÃ¡t hiá»‡n: {np.sum(outlier_mask)}\")\n",
    "print(f\"   â€¢ Tá»· lá»‡ outliers: {np.sum(outlier_mask)/len(data_no_missing)*100:.2f}%\")\n",
    "print(f\"   â€¢ Dá»¯ liá»‡u trÆ°á»›c: {len(data_no_missing)} samples\")\n",
    "print(f\"   â€¢ Dá»¯ liá»‡u sau: {len(data_no_outliers)} samples\")\n",
    "print(f\"   â€¢ Samples bá»‹ loáº¡i bá»: {len(data_no_missing) - len(data_no_outliers)}\")\n",
    "\n",
    "# Visualize outliers cho cÃ¡c biáº¿n numeric\n",
    "numeric_columns = [col for col in data_no_missing.dtype.names \n",
    "                  if np.issubdtype(data_no_missing[col].dtype, np.number)\n",
    "                  and col not in ['enrollee_id', 'target']]\n",
    "\n",
    "print(f\"\\nğŸ“ˆ VISUALIZE OUTLIERS CHO {len(numeric_columns)} BIáº¾N NUMERIC:\")\n",
    "\n",
    "setup_plot_style()\n",
    "fig, axes = plt.subplots(1, len(numeric_columns), figsize=(15, 5))\n",
    "\n",
    "if len(numeric_columns) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, col in enumerate(numeric_columns):\n",
    "    if idx < len(axes):\n",
    "        clean_data = data_no_missing[col][~np.isnan(data_no_missing[col])]\n",
    "        axes[idx].boxplot(clean_data)\n",
    "        axes[idx].set_title(f'{col} - Before')\n",
    "        axes[idx].set_ylabel('Values')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show distribution sau khi remove outliers\n",
    "print(\"\\nğŸ“Š PHÃ‚N PHá»I SAU KHI REMOVE OUTLIERS:\")\n",
    "for col in numeric_columns[:3]:  # Hiá»ƒn thá»‹ 3 biáº¿n Ä‘áº§u\n",
    "    before_data = data_no_missing[col][~np.isnan(data_no_missing[col])]\n",
    "    after_data = data_no_outliers[col][~np.isnan(data_no_outliers[col])]\n",
    "    \n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"   Before: mean={np.mean(before_data):.3f}, std={np.std(before_data):.3f}\")\n",
    "    print(f\"   After:  mean={np.mean(after_data):.3f}, std={np.std(after_data):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "print(\"=== FEATURE ENGINEERING ===\")\n",
    "\n",
    "engineered_data = feature_engineering(data_no_outliers)\n",
    "\n",
    "# Kiá»ƒm tra cÃ¡c features má»›i Ä‘Æ°á»£c táº¡o\n",
    "original_features = data_no_outliers.dtype.names\n",
    "new_features = [col for col in engineered_data.dtype.names if col not in original_features]\n",
    "\n",
    "print(f\"ğŸ¯ FEATURES Má»šI ÄÆ¯á»¢C Táº O: {len(new_features)}\")\n",
    "for feature in new_features:\n",
    "    print(f\"   â€¢ {feature}\")\n",
    "\n",
    "# Hiá»ƒn thá»‹ thÃ´ng tin vá» features má»›i\n",
    "print(f\"\\nğŸ“Š THÃ”NG TIN FEATURES Má»šI:\")\n",
    "for feature in new_features:\n",
    "    if feature in engineered_data.dtype.names:\n",
    "        feature_data = engineered_data[feature]\n",
    "        if np.issubdtype(feature_data.dtype, np.number):\n",
    "            clean_data = feature_data[~np.isnan(feature_data)]\n",
    "            if len(clean_data) > 0:\n",
    "                print(f\"\\n{feature}:\")\n",
    "                print(f\"   â€¢ Min: {np.min(clean_data):.3f}\")\n",
    "                print(f\"   â€¢ Max: {np.max(clean_data):.3f}\")\n",
    "                print(f\"   â€¢ Mean: {np.mean(clean_data):.3f}\")\n",
    "                print(f\"   â€¢ Std: {np.std(clean_data):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Encode Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Categorical Features\n",
    "print(\"=== ENCODE CATEGORICAL FEATURES ===\")\n",
    "\n",
    "encoded_data, encoders = encode_categorical_features(engineered_data)\n",
    "\n",
    "print(f\"ğŸ“Š Káº¾T QUáº¢ ENCODING:\")\n",
    "print(f\"   â€¢ Sá»‘ lÆ°á»£ng categorical features Ä‘Æ°á»£c encode: {len(encoders)}\")\n",
    "print(f\"   â€¢ CÃ¡c biáº¿n Ä‘Æ°á»£c encode: {list(encoders.keys())}\")\n",
    "\n",
    "# Hiá»ƒn thá»‹ encoding mapping\n",
    "print(f\"\\nğŸ”¤ ENCODING MAPPING (5 categories Ä‘áº§u tiÃªn má»—i biáº¿n):\")\n",
    "for col_name, encoder_dict in encoders.items():\n",
    "    print(f\"\\n{col_name}:\")\n",
    "    items = list(encoder_dict.items())\n",
    "    for category, code in items[:5]:\n",
    "        print(f\"   â€¢ {category:20} â†’ {code}\")\n",
    "    if len(items) > 5:\n",
    "        print(f\"   â€¢ ... vÃ  {len(items) - 5} categories khÃ¡c\")\n",
    "\n",
    "# Kiá»ƒm tra dá»¯ liá»‡u sau encode\n",
    "print(f\"\\nâœ… KIá»‚M TRA Dá»® LIá»†U SAU ENCODE:\")\n",
    "print(f\"   â€¢ Shape: {encoded_data.shape}\")\n",
    "print(f\"   â€¢ Columns: {len(encoded_data.dtype.names)}\")\n",
    "print(f\"   â€¢ Data types:\")\n",
    "numeric_count = 0\n",
    "for col in encoded_data.dtype.names:\n",
    "    if np.issubdtype(encoded_data[col].dtype, np.number):\n",
    "        numeric_count += 1\n",
    "print(f\"   â€¢ Numeric columns: {numeric_count}/{len(encoded_data.dtype.names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Chuáº©n HÃ³a Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chuáº©n hÃ³a Features\n",
    "print(\"=== CHUáº¨N HÃ“A FEATURES ===\")\n",
    "\n",
    "# So sÃ¡nh cÃ¡c phÆ°Æ¡ng phÃ¡p chuáº©n hÃ³a\n",
    "methods = ['minmax', 'zscore', 'decimal']\n",
    "normalized_datasets = {}\n",
    "\n",
    "print(\"ğŸ” SO SÃNH CÃC PHÆ¯Æ NG PHÃP CHUáº¨N HÃ“A:\")\n",
    "for method in methods:\n",
    "    print(f\"\\nğŸ“Š PhÆ°Æ¡ng phÃ¡p: {method.upper()}\")\n",
    "    normalized_data = normalize_features(encoded_data, method=method)\n",
    "    normalized_datasets[method] = normalized_data\n",
    "    \n",
    "    # Láº¥y cÃ¡c biáº¿n numeric (trá»« ID vÃ  target)\n",
    "    numeric_cols = [col for col in normalized_data.dtype.names \n",
    "                   if np.issubdtype(normalized_data[col].dtype, np.number)\n",
    "                   and col not in ['enrollee_id', 'target']]\n",
    "    \n",
    "    if numeric_cols:\n",
    "        sample_col = numeric_cols[0]\n",
    "        col_data = normalized_data[sample_col]\n",
    "        clean_data = col_data[~np.isnan(col_data)]\n",
    "        \n",
    "        if len(clean_data) > 0:\n",
    "            print(f\"   â€¢ {sample_col}:\")\n",
    "            print(f\"     Min: {np.min(clean_data):.3f}\")\n",
    "            print(f\"     Max: {np.max(clean_data):.3f}\")\n",
    "            print(f\"     Mean: {np.mean(clean_data):.3f}\")\n",
    "            print(f\"     Std: {np.std(clean_data):.3f}\")\n",
    "\n",
    "# Chá»n phÆ°Æ¡ng phÃ¡p minmax cho cÃ¡c bÆ°á»›c tiáº¿p theo\n",
    "final_data = normalized_datasets['minmax']\n",
    "print(f\"\\nâœ… ÄÃƒ CHá»ŒN PHÆ¯Æ NG PHÃP: MIN-MAX NORMALIZATION\")\n",
    "print(f\"   â€¢ LÃ½ do: ÄÆ°a táº¥t cáº£ features vá» cÃ¹ng khoáº£ng [0, 1]\")\n",
    "print(f\"   â€¢ PhÃ¹ há»£p cho nhiá»u thuáº­t toÃ¡n machine learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. PhÃ¢n TÃ­ch TÆ°Æ¡ng Quan Sau Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PhÃ¢n tÃ­ch tÆ°Æ¡ng quan sau preprocessing\n",
    "print(\"=== PHÃ‚N TÃCH TÆ¯Æ NG QUAN SAU PREPROCESSING ===\")\n",
    "\n",
    "# Láº¥y cÃ¡c biáº¿n numeric\n",
    "numeric_columns = [col for col in final_data.dtype.names \n",
    "                  if np.issubdtype(final_data[col].dtype, np.number)\n",
    "                  and col not in ['enrollee_id']]\n",
    "\n",
    "print(f\"ğŸ“Š CÃC BIáº¾N NUMERIC Äá»‚ PHÃ‚N TÃCH: {len(numeric_columns)}\")\n",
    "print(f\"   â€¢ {numeric_columns}\")\n",
    "\n",
    "# Váº½ heatmap tÆ°Æ¡ng quan\n",
    "plot_correlation_heatmap(final_data, numeric_columns)\n",
    "\n",
    "# PhÃ¢n tÃ­ch tÆ°Æ¡ng quan vá»›i target\n",
    "if 'target' in final_data.dtype.names:\n",
    "    print(\"\\nğŸ” TÆ¯Æ NG QUAN Vá»šI BIáº¾N TARGET:\")\n",
    "    target_correlations = []\n",
    "    \n",
    "    for col in numeric_columns:\n",
    "        if col != 'target':\n",
    "            # TÃ­nh correlation\n",
    "            x_data = final_data[col]\n",
    "            y_data = final_data['target']\n",
    "            \n",
    "            mask = ~(np.isnan(x_data) | np.isnan(y_data))\n",
    "            x_clean = x_data[mask]\n",
    "            y_clean = y_data[mask]\n",
    "            \n",
    "            if len(x_clean) > 1:\n",
    "                correlation = np.corrcoef(x_clean, y_clean)[0, 1]\n",
    "                target_correlations.append((col, abs(correlation), correlation))\n",
    "    \n",
    "    # Sáº¯p xáº¿p theo absolute correlation\n",
    "    target_correlations.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"\\nğŸ“ˆ TOP FEATURES CÃ“ TÆ¯Æ NG QUAN CAO NHáº¤T Vá»šI TARGET:\")\n",
    "    for col, abs_corr, corr in target_correlations[:10]:\n",
    "        direction = \"dÆ°Æ¡ng\" if corr > 0 else \"Ã¢m\"\n",
    "        print(f\"   â€¢ {col:25}: {corr:7.3f} ({direction})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. LÆ°u Dá»¯ Liá»‡u ÄÃ£ Xá»­ LÃ½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LÆ°u dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½ (KHÃ”NG DÃ™NG PICKLE)\n",
    "print(\"=== LÆ¯U Dá»® LIá»†U ÄÃƒ Xá»¬ LÃ (CSV + JSON) ===\")\n",
    "\n",
    "# Táº¡o thÆ° má»¥c náº¿u chÆ°a tá»“n táº¡i\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "def save_structured_array_to_csv(data, filename):\n",
    "    \"\"\"LÆ°u structured numpy array thÃ nh file CSV chá»‰ dÃ¹ng NumPy\"\"\"\n",
    "    \n",
    "    # Táº¡o header\n",
    "    header = ','.join(data.dtype.names)\n",
    "    \n",
    "    # Táº¡o cÃ¡c dÃ²ng dá»¯ liá»‡u\n",
    "    lines = []\n",
    "    for row in data:\n",
    "        row_data = []\n",
    "        for field in data.dtype.names:\n",
    "            value = row[field]\n",
    "            \n",
    "            # Xá»­ lÃ½ cÃ¡c loáº¡i dá»¯ liá»‡u\n",
    "            if isinstance(value, (np.float32, np.float64, np.int32, np.int64)):\n",
    "                # Kiá»ƒm tra numeric values\n",
    "                if np.isnan(value):\n",
    "                    row_data.append('')\n",
    "                elif isinstance(value, (np.float32, np.float64)):\n",
    "                    row_data.append(f'{value:.6f}')\n",
    "                else:  # integer types\n",
    "                    row_data.append(str(value))\n",
    "            else:\n",
    "                # String values\n",
    "                if value in ['', 'nan', 'NaN', 'None']:\n",
    "                    row_data.append('')\n",
    "                else:\n",
    "                    # Escape commas vÃ  quotes trong string\n",
    "                    str_value = str(value).replace('\"', '\"\"')\n",
    "                    if ',' in str_value or '\"' in str_value:\n",
    "                        row_data.append(f'\"{str_value}\"')\n",
    "                    else:\n",
    "                        row_data.append(str_value)\n",
    "        lines.append(','.join(row_data))\n",
    "    \n",
    "    # Ghi file\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(header + '\\n')\n",
    "        f.write('\\n'.join(lines))\n",
    "    \n",
    "    print(f\"   âœ… ÄÃ£ lÆ°u {len(data)} dÃ²ng vÃ o {filename}\")\n",
    "\n",
    "# LÆ°u dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½ dÆ°á»›i dáº¡ng CSV\n",
    "processed_csv_path = '../data/processed/processed_data.csv'\n",
    "save_structured_array_to_csv(final_data, processed_csv_path)\n",
    "\n",
    "# LÆ°u dá»¯ liá»‡u gá»‘c Ä‘á»ƒ so sÃ¡nh\n",
    "raw_csv_path = '../data/processed/raw_data_comparison.csv'\n",
    "save_structured_array_to_csv(raw_data, raw_csv_path)\n",
    "\n",
    "# LÆ°u metadata dÆ°á»›i dáº¡ng JSON (thay vÃ¬ pickle)\n",
    "import json\n",
    "\n",
    "# Chuáº©n bá»‹ metadata\n",
    "metadata = {\n",
    "    'feature_names': list(final_data.dtype.names),\n",
    "    'preprocessing_steps': [\n",
    "        'missing_value_imputation',\n",
    "        'outlier_removal_iqr', \n",
    "        'feature_engineering',\n",
    "        'categorical_encoding',\n",
    "        'minmax_normalization'\n",
    "    ],\n",
    "    'dataset_info': {\n",
    "        'original_shape': [len(raw_data), len(raw_data.dtype.names)],\n",
    "        'processed_shape': [len(final_data), len(final_data.dtype.names)],\n",
    "        'outliers_removed': int(np.sum(outlier_mask)) if 'outlier_mask' in locals() else 0,\n",
    "        'new_features_created': len(new_features) if 'new_features' in locals() else 0\n",
    "    },\n",
    "    'encoding_mappings': {}\n",
    "}\n",
    "\n",
    "# ThÃªm encoding mappings náº¿u cÃ³\n",
    "if 'encoders' in locals():\n",
    "    # Convert encoders to JSON-serializable format\n",
    "    for col_name, encoder_dict in encoders.items():\n",
    "        metadata['encoding_mappings'][col_name] = {str(k): int(v) for k, v in encoder_dict.items()}\n",
    "\n",
    "# LÆ°u metadata dÆ°á»›i dáº¡ng JSON\n",
    "metadata_path = '../data/processed/preprocessing_metadata.json'\n",
    "with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# LÆ°u thÃ´ng tin features má»›i (náº¿u cÃ³)\n",
    "new_features_info = {}\n",
    "if 'new_features' in locals():\n",
    "    for feature in new_features:\n",
    "        if feature in final_data.dtype.names:\n",
    "            feature_data = final_data[feature]\n",
    "            if np.issubdtype(feature_data.dtype, np.number):\n",
    "                clean_data = feature_data[~np.isnan(feature_data)]\n",
    "                if len(clean_data) > 0:\n",
    "                    new_features_info[feature] = {\n",
    "                        'min': float(np.min(clean_data)),\n",
    "                        'max': float(np.max(clean_data)),\n",
    "                        'mean': float(np.mean(clean_data)),\n",
    "                        'std': float(np.std(clean_data))\n",
    "                    }\n",
    "\n",
    "    # LÆ°u thÃ´ng tin features má»›i\n",
    "    new_features_path = '../data/processed/new_features_info.json'\n",
    "    with open(new_features_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(new_features_info, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… Dá»® LIá»†U ÄÃƒ Xá»¬ LÃ ÄÆ¯á»¢C LÆ¯U Táº I: {processed_csv_path}\")\n",
    "print(f\"âœ… Dá»® LIá»†U Gá»C (SO SÃNH): {raw_csv_path}\")\n",
    "print(f\"âœ… METADATA ÄÆ¯á»¢C LÆ¯U Táº I: {metadata_path}\")\n",
    "if 'new_features' in locals() and new_features:\n",
    "    print(f\"âœ… THÃ”NG TIN FEATURES Má»šI: {new_features_path}\")\n",
    "\n",
    "# Hiá»ƒn thá»‹ summary\n",
    "print(f\"\\nğŸ“Š TÃ“M Táº®T QUÃ TRÃŒNH PREPROCESSING:\")\n",
    "print(f\"   â€¢ Dá»¯ liá»‡u gá»‘c:        {len(raw_data)} samples, {len(raw_data.dtype.names)} features\")\n",
    "print(f\"   â€¢ Dá»¯ liá»‡u sau xá»­ lÃ½: {len(final_data)} samples, {len(final_data.dtype.names)} features\")\n",
    "if 'new_features' in locals():\n",
    "    print(f\"   â€¢ Features má»›i:      {len(new_features)}\")\n",
    "if 'outlier_mask' in locals():\n",
    "    print(f\"   â€¢ Outliers removed:  {np.sum(outlier_mask)}\")\n",
    "if 'encoders' in locals():\n",
    "    print(f\"   â€¢ Encoded variables: {len(encoders)}\")\n",
    "\n",
    "# Hiá»ƒn thá»‹ thÃ´ng tin file\n",
    "print(f\"\\nğŸ“ THÃ”NG TIN FILE ÄÃƒ LÆ¯U:\")\n",
    "import os\n",
    "if os.path.exists(processed_csv_path):\n",
    "    file_size_mb = os.path.getsize(processed_csv_path) / (1024 * 1024)\n",
    "    print(f\"   â€¢ processed_data.csv: {file_size_mb:.2f} MB\")\n",
    "    print(f\"   â€¢ Äá»‹nh dáº¡ng: CSV (cÃ³ thá»ƒ má»Ÿ báº±ng Excel, Pandas, etc.)\")\n",
    "    print(f\"   â€¢ Encoding: UTF-8\")\n",
    "    print(f\"   â€¢ Sá»‘ cá»™t: {len(final_data.dtype.names)}\")\n",
    "    print(f\"   â€¢ Sá»‘ dÃ²ng: {len(final_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Tá»•ng Káº¿t Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tá»•ng káº¿t preprocessing\n",
    "print(\"=== Tá»”NG Káº¾T PREPROCESSING ===\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Thá»‘ng kÃª mÃ´ táº£ cuá»‘i cÃ¹ng\n",
    "final_stats = compute_descriptive_statistics(final_data)\n",
    "\n",
    "print(\"ğŸ“ˆ THá»NG KÃŠ MÃ” Táº¢ SAU PREPROCESSING:\")\n",
    "numeric_features = [col for col in final_data.dtype.names \n",
    "                   if np.issubdtype(final_data[col].dtype, np.number)\n",
    "                   and col not in ['enrollee_id']]\n",
    "\n",
    "print(f\"\\nğŸ”¢ CÃC FEATURES NUMERIC ({len(numeric_features)}):\")\n",
    "for col in numeric_features[:8]:  # Hiá»ƒn thá»‹ 8 features Ä‘áº§u\n",
    "    if col in final_stats and final_stats[col]:\n",
    "        stats = final_stats[col]\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"   â€¢ Count:  {stats.get('count', 'N/A')}\")\n",
    "        print(f\"   â€¢ Mean:   {stats.get('mean', 'N/A'):.3f}\")\n",
    "        print(f\"   â€¢ Std:    {stats.get('std', 'N/A'):.3f}\")\n",
    "        print(f\"   â€¢ Range:  [{stats.get('min', 'N/A'):.3f}, {stats.get('max', 'N/A'):.3f}]\")\n",
    "\n",
    "if len(numeric_features) > 8:\n",
    "    print(f\"\\n... vÃ  {len(numeric_features) - 8} features numeric khÃ¡c\")\n",
    "\n",
    "# ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng dá»¯ liá»‡u\n",
    "print(f\"\\nâœ… ÄÃNH GIÃ CHáº¤T LÆ¯á»¢NG Dá»® LIá»†U:\")\n",
    "print(\"   âœ“ Missing values:     ÄÃ£ Ä‘Æ°á»£c xá»­ lÃ½ hoÃ n toÃ n\")\n",
    "print(\"   âœ“ Outliers:           ÄÃ£ Ä‘Æ°á»£c phÃ¡t hiá»‡n vÃ  xá»­ lÃ½\")\n",
    "print(\"   âœ“ Categorical vars:   ÄÃ£ Ä‘Æ°á»£c encode thÃ nh numeric\") \n",
    "print(\"   âœ“ Features:           ÄÃ£ Ä‘Æ°á»£c chuáº©n hÃ³a vá» [0, 1]\")\n",
    "if 'new_features' in locals() and new_features:\n",
    "    print(\"   âœ“ New features:       ÄÃ£ Ä‘Æ°á»£c táº¡o Ä‘á»ƒ cáº£i thiá»‡n predictive power\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ Dá»® LIá»†U ÄÃƒ ÄÆ¯á»¢C LÆ¯U DÆ¯á»šI Dáº NG CSV:\")\n",
    "print(\"   âœ“ processed_data.csv - Dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½\")\n",
    "print(\"   âœ“ raw_data_comparison.csv - Dá»¯ liá»‡u gá»‘c Ä‘á»ƒ so sÃ¡nh\") \n",
    "print(\"   âœ“ preprocessing_metadata.json - ThÃ´ng tin preprocessing\")\n",
    "if 'new_features' in locals() and new_features:\n",
    "    print(\"   âœ“ new_features_info.json - ThÃ´ng tin features má»›i\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Dá»® LIá»†U ÄÃƒ Sáº´N SÃ€NG CHO MODELING!\")\n",
    "print(\"   â†’ Chuyá»ƒn sang notebook: 03_modeling.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
